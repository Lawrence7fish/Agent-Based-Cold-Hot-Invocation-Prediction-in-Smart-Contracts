{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed, dump\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, BatchNormalization, RepeatVector, MultiHeadAttention, Concatenate, Dense\n",
    "from tensorflow.keras.layers import GRU, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设定\n",
    "threshold = 20\n",
    "window = 20\n",
    "slide = 1\n",
    "valid_ratio = 0.2\n",
    "future_label = 5\n",
    "feature_cols = ['gas_mean', 'gasUsed_mean', 'counts','hour', 'minute', 'second', 'is_exist']\n",
    "\n",
    "# 数据加载与预处理（解决 SettingWithCopyWarning）\n",
    "def load_and_preprocess(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data[data['isError'] == 0].copy()\n",
    "    data['type'], _ = pd.factorize(data['type'])\n",
    "    mask = data['type'] == 1\n",
    "    data['to'] = np.where(mask, data['contractAddress'], data['to'])\n",
    "    data = data.drop(columns=['type', 'contractAddress'])\n",
    "    \n",
    "    # 聚合数据\n",
    "    data = data.groupby(['blockNumber', 'timeStamp', 'to']).agg(\n",
    "        gas_mean=('gas', 'mean'),\n",
    "        gasUsed_mean=('gasUsed', 'mean'),\n",
    "        counts=('gas', 'size')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 过滤低频合约\n",
    "    sum_data = data.groupby('to')['counts'].sum().reset_index()\n",
    "    filtered_data = data[data['to'].isin(sum_data[sum_data['counts'] > threshold]['to'])].copy()\n",
    "    \n",
    "    # 时间特征\n",
    "    filtered_data['timeStamp'] = pd.to_datetime(filtered_data['timeStamp'], unit='s')\n",
    "    filtered_data['hour'] = filtered_data['timeStamp'].dt.hour\n",
    "    filtered_data['minute'] = filtered_data['timeStamp'].dt.minute\n",
    "    filtered_data['second'] = filtered_data['timeStamp'].dt.second\n",
    "\n",
    "    filtered_data['is_exist'] = 1\n",
    "    return filtered_data.drop(columns=['timeStamp']).sort_values('blockNumber')\n",
    "\n",
    "# 加载数据\n",
    "filtered_data = load_and_preprocess(\"dataset.csv\")\n",
    "\n",
    "split_idx1 = int(0.8 * len(filtered_data))\n",
    "split_idx2 = int(0.9 * len(filtered_data))\n",
    "\n",
    "train_data = filtered_data.iloc[:split_idx1]\n",
    "valid_data = filtered_data.iloc[split_idx1:split_idx2]\n",
    "test_data = filtered_data.iloc[split_idx2:]\n",
    "\n",
    "group_train_data = train_data.groupby(['to'])\n",
    "group_valid_data = valid_data.groupby(['to'])\n",
    "group_test_data = test_data.groupby(['to'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 917/917 [00:35<00:00, 26.07it/s]\n",
      "Valid: 100%|██████████| 801/801 [00:03<00:00, 250.74it/s]\n",
      "Test: 100%|██████████| 799/799 [00:03<00:00, 243.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136941 136941\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(group):\n",
    "    blocks = group['blockNumber'].values.astype(int)\n",
    "    counts = group['counts'].values\n",
    "\n",
    "    i = 0\n",
    "    n_blocks = len(blocks)\n",
    "\n",
    "    feature = []\n",
    "    labels = []\n",
    "    while i < n_blocks - 1:\n",
    "        # 当前窗口的起始区块号\n",
    "        start_block = blocks[i]\n",
    "        end_block = start_block + window\n",
    "\n",
    "        # 使用二分查找确定窗口结束位置\n",
    "        j = np.searchsorted(blocks, end_block, side='right')\n",
    "\n",
    "        # 窗口内实际存在的区块索引\n",
    "        window_mask = (blocks >= start_block) & (blocks < end_block)\n",
    "        window_blocks = blocks[window_mask]\n",
    "        window_counts = counts[window_mask]\n",
    "\n",
    "        # 填充完整的区块序列（即使没有交易）\n",
    "        full_features = np.zeros((window, len(feature_cols)))\n",
    "        full_counts = np.zeros(window)\n",
    "        existing_indices = window_blocks - start_block\n",
    "        full_features[existing_indices] = group[window_mask][feature_cols].values\n",
    "        full_counts[existing_indices] = window_counts\n",
    "\n",
    "        # 有效性检查\n",
    "        valid_ratio_actual = np.sum(full_counts > 0) / window\n",
    "        if valid_ratio_actual < valid_ratio:\n",
    "            # 动态调整起始点：跳到下一个有效位置\n",
    "            next_block_idx = j  # 默认跳到当前窗口结束位置\n",
    "            if j < n_blocks:\n",
    "                # 计算下一个可能有效的位置：下一个区块的前window个位置\n",
    "                next_valid_start = max(blocks[j] - window + 1, start_block + 1)\n",
    "                next_block_idx = np.searchsorted(blocks, next_valid_start, side='left')\n",
    "\n",
    "            i = next_block_idx\n",
    "            continue\n",
    "\n",
    "        # 生成标签：用未来future_label个区块作为标签\n",
    "        label = np.arange(future_label)\n",
    "        future_start = end_block\n",
    "        future_end = future_start + future_label\n",
    "        future_mask = (blocks >= future_start) & (blocks < future_end)\n",
    "        future_blocks = blocks[future_mask]\n",
    "        future_counts = counts[future_mask]\n",
    "        future_existing = future_blocks - end_block\n",
    "        label[future_existing] = future_counts\n",
    "\n",
    "        # 保存序列和标签\n",
    "        feature.append(full_features)\n",
    "        labels.append(label)\n",
    "        # 滑动步长（至少滑动1个区块）\n",
    "        i += max(slide, 1)\n",
    "\n",
    "    return feature, labels\n",
    "\n",
    "create_train = Parallel(n_jobs=-1)(\n",
    "    delayed(create_sequences)(group) for _, group in tqdm(group_train_data, desc='Train')\n",
    ")\n",
    "\n",
    "create_valid = Parallel(n_jobs=-1)(\n",
    "    delayed(create_sequences)(group) for _, group in tqdm(group_valid_data, desc='Valid')\n",
    ")\n",
    "\n",
    "create_test = Parallel(n_jobs=-1)(\n",
    "    delayed(create_sequences)(group) for _, group in tqdm(group_test_data, desc='Test')\n",
    ")\n",
    "\n",
    "# 创建时间序列数据集\n",
    "X_train, y_train = [], []\n",
    "X_valid, y_valid = [], []\n",
    "X_test, y_test = [], []\n",
    "for train in create_train:\n",
    "    X_train.extend(train[0])\n",
    "    y_train.extend(train[1])\n",
    "for valid in create_valid:\n",
    "    X_valid.extend(valid[0])\n",
    "    y_valid.extend(valid[1])\n",
    "for test in create_test:\n",
    "    X_test.extend(test[0])\n",
    "    y_test.extend(test[1])\n",
    "\n",
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136941, 20, 7) (136941, 5)\n",
      "(136941, 5)\n",
      "Epoch 1/50\n",
      "2140/2140 [==============================] - 56s 22ms/step - loss: 0.4566 - mae: 0.3233 - val_loss: 0.4564 - val_mae: 0.3479 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2140/2140 [==============================] - 42s 20ms/step - loss: 0.4130 - mae: 0.3154 - val_loss: 0.4470 - val_mae: 0.3415 - lr: 9.0000e-04\n",
      "Epoch 3/50\n",
      "2140/2140 [==============================] - 45s 21ms/step - loss: 0.4095 - mae: 0.3132 - val_loss: 0.4479 - val_mae: 0.3422 - lr: 8.1000e-04\n",
      "Epoch 4/50\n",
      "2140/2140 [==============================] - 44s 20ms/step - loss: 0.4076 - mae: 0.3120 - val_loss: 0.4541 - val_mae: 0.3472 - lr: 7.2900e-04\n",
      "Epoch 5/50\n",
      "2140/2140 [==============================] - 45s 21ms/step - loss: 0.4064 - mae: 0.3113 - val_loss: 0.4434 - val_mae: 0.3392 - lr: 6.5610e-04\n",
      "Epoch 6/50\n",
      "2140/2140 [==============================] - 46s 21ms/step - loss: 0.4055 - mae: 0.3106 - val_loss: 0.4491 - val_mae: 0.3437 - lr: 5.9049e-04\n",
      "Epoch 7/50\n",
      "2140/2140 [==============================] - 45s 21ms/step - loss: 0.4042 - mae: 0.3098 - val_loss: 0.4431 - val_mae: 0.3392 - lr: 5.3144e-04\n",
      "Epoch 8/50\n",
      "2140/2140 [==============================] - 45s 21ms/step - loss: 0.4036 - mae: 0.3095 - val_loss: 0.4452 - val_mae: 0.3412 - lr: 4.7830e-04\n",
      "Epoch 9/50\n",
      "2140/2140 [==============================] - 42s 20ms/step - loss: 0.4030 - mae: 0.3091 - val_loss: 0.4506 - val_mae: 0.3450 - lr: 4.3047e-04\n",
      "Epoch 10/50\n",
      "2140/2140 [==============================] - 43s 20ms/step - loss: 0.4025 - mae: 0.3088 - val_loss: 0.4459 - val_mae: 0.3415 - lr: 3.8742e-04\n",
      "Epoch 11/50\n",
      "2140/2140 [==============================] - 41s 19ms/step - loss: 0.4017 - mae: 0.3083 - val_loss: 0.4470 - val_mae: 0.3423 - lr: 3.4868e-04\n",
      "Epoch 12/50\n",
      "2140/2140 [==============================] - 41s 19ms/step - loss: 0.4016 - mae: 0.3082 - val_loss: 0.4443 - val_mae: 0.3402 - lr: 3.1381e-04\n",
      "Epoch 13/50\n",
      "2140/2140 [==============================] - 42s 20ms/step - loss: 0.4010 - mae: 0.3078 - val_loss: 0.4486 - val_mae: 0.3436 - lr: 2.8243e-04\n",
      "Epoch 14/50\n",
      "2140/2140 [==============================] - 42s 20ms/step - loss: 0.4005 - mae: 0.3074 - val_loss: 0.4441 - val_mae: 0.3401 - lr: 2.5419e-04\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# 将三维数据展平为二维进行标准化\n",
    "n_samples, timesteps, n_features = X_train.shape\n",
    "X_train_2d = X_train.reshape(-1, n_features)\n",
    "X_valid_2d = X_valid.reshape(-1, n_features)\n",
    "X_test_2d = X_test.reshape(-1, n_features)\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_2d).reshape(n_samples, timesteps, n_features)\n",
    "X_valid_scaled = scaler_X.transform(X_valid_2d).reshape(X_valid.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test_2d).reshape(X_test.shape)\n",
    "\n",
    "# 标签标准化\n",
    "y_train = np.array(y_train).reshape(-1, future_label)  # 形状变为 (n_samples, 5)\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_valid_scaled = scaler_y.transform(y_valid)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "def create_model(window, future_label, feature_dim):\n",
    "    # 编码器部分\n",
    "    encoder_inputs = Input(shape=(window, feature_dim))\n",
    "    x = LSTM(128, return_sequences=True)(encoder_inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)  \n",
    "    x = Dropout(0.3)(x)\n",
    "    encoder_outputs = x\n",
    "\n",
    "    # 解码器部分\n",
    "    decoder_input = RepeatVector(future_label)(encoder_outputs[:, -1, :])\n",
    "    decoder_lstm = LSTM(64, return_sequences=True)(decoder_input)  # 解码器LSTM输出 (batch_size, 5, 64)\n",
    "\n",
    "   # 多头注意力\n",
    "    attention = MultiHeadAttention(num_heads=8, key_dim=64)(\n",
    "        query=decoder_lstm, \n",
    "        value=encoder_outputs, \n",
    "        key=encoder_outputs)\n",
    "    merged = Concatenate()([decoder_lstm, attention])\n",
    "    merged = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(merged)\n",
    "    outputs = Dense(1)(merged)\n",
    "    model = Model(inputs=encoder_inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# def create_model(window, future_label, feature_dim):\n",
    "#     # 编码器部分（GRU替代）\n",
    "#     encoder_inputs = Input(shape=(window, feature_dim))\n",
    "#     x = GRU(128, return_sequences=True)(encoder_inputs)\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = GRU(64, return_sequences=True)(x)  # 第二层GRU\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     encoder_outputs = x\n",
    "\n",
    "#     # 保持原有注意力机制结构\n",
    "#     decoder_input = RepeatVector(future_label)(encoder_outputs[:, -1, :])\n",
    "#     decoder_gru = GRU(64, return_sequences=True)(decoder_input)\n",
    "    \n",
    "#     attention = MultiHeadAttention(num_heads=4, key_dim=64)(\n",
    "#         query=decoder_gru,\n",
    "#         value=encoder_outputs,\n",
    "#         key=encoder_outputs)\n",
    "#     merged = Concatenate()([decoder_gru, attention])\n",
    "#     merged = Dense(64, activation='relu', \n",
    "#                  kernel_regularizer=regularizers.l2(0.01))(merged)\n",
    "    \n",
    "#     outputs = Dense(1)(merged)\n",
    "#     model = Model(inputs=encoder_inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# def create_model(window, future_label, feature_dim):\n",
    "#     # 双向LSTM\n",
    "#     encoder_inputs = Input(shape=(window, feature_dim))\n",
    "#     x = Bidirectional(LSTM(128, return_sequences=True))(encoder_inputs)  # 输出维度256\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Bidirectional(LSTM(64, return_sequences=True))(x)  # 输出维度128\n",
    "#     x = Dropout(0.3)(x)\n",
    "#     encoder_outputs = x  # 形状(batch, timesteps, 128)\n",
    "\n",
    "#     # 解码器部分（保持单向LSTM）\n",
    "#     decoder_input = RepeatVector(future_label)(encoder_outputs[:, -1, :])  # 取最后一个时间步\n",
    "#     decoder_lstm = LSTM(128, return_sequences=True)(decoder_input)  # 增大单元数以匹配编码器输出\n",
    "    \n",
    "#     # 调整注意力机制维度\n",
    "#     attention = MultiHeadAttention(\n",
    "#         num_heads=8,  # 增加注意力头数以匹配更高维度\n",
    "#         key_dim=32    # 减小单个头维度防止参数膨胀\n",
    "#     )(query=decoder_lstm, value=encoder_outputs, key=encoder_outputs)\n",
    "    \n",
    "#     merged = Concatenate()([decoder_lstm, attention])\n",
    "#     merged = Dense(128, activation='swish',  # 增大维度并改用swish激活\n",
    "#                  kernel_regularizer=regularizers.l1_l2(0.01, 0.01))(merged)\n",
    "    \n",
    "#     outputs = Dense(1)(merged)\n",
    "#     model = Model(inputs=encoder_inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "model = create_model(window, future_label, len(feature_cols))\n",
    "\n",
    "step_weights = np.array([2, 1.5, 1.2, 1, 1])\n",
    "\n",
    "def weighted_mae(y_true, y_pred):\n",
    "    # 扩展权重到匹配形状\n",
    "    weights = K.constant(step_weights.reshape(1, -1, 1), dtype=tf.float32)\n",
    "    \n",
    "    # 计算加权绝对误差\n",
    "    absolute_errors = K.abs(y_true - y_pred)\n",
    "\n",
    "    weighted_errors = weights * absolute_errors\n",
    "    \n",
    "    # 求平均值\n",
    "    return K.mean(weighted_errors)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=weighted_mae,\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled.reshape(-1, future_label, 1),  # 调整为 (samples, 5, 1)\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_valid_scaled, y_valid_scaled.reshape(-1, future_label, 1)),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=7),\n",
    "        tf.keras.callbacks.LearningRateScheduler(\n",
    "            lambda epoch: 0.001 * (0.9 ** epoch ))\n",
    "        ]  # 学习率衰减\n",
    ")\n",
    "\n",
    "# dump(scaler_X, 'scaler_X.joblib')\n",
    "# dump(scaler_y, 'scaler_y.joblib')\n",
    "# model.save('lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524/524 [==============================] - 5s 7ms/step\n",
      "(16753, 5) (16753, 5)\n",
      "Overall MAE: 0.97\n",
      "Overall MSE: 5.56\n",
      "Step 1 MAE: 1.06, MSE: 5.82\n",
      "Step 2 MAE: 0.76, MSE: 5.50\n",
      "Step 3 MAE: 0.77, MSE: 4.87\n",
      "Step 4 MAE: 0.97, MSE: 5.29\n",
      "Step 5 MAE: 1.27, MSE: 6.34\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    # 展平维度计算整体指标\n",
    "    flat_true = y_true.flatten()\n",
    "    flat_pred = y_pred.flatten()\n",
    "\n",
    "    # 平均绝对误差和均方误差\n",
    "    print(f\"Overall MAE: {mean_absolute_error(flat_true, flat_pred):.2f}\")\n",
    "    print(f\"Overall MSE: {mean_squared_error(flat_true, flat_pred):.2f}\")\n",
    "\n",
    "    # 分步长计算指标\n",
    "    for step in range(future_label):\n",
    "        step_mae = mean_absolute_error(y_true[:, step], y_pred[:, step])\n",
    "        step_mse = mean_squared_error(y_true[:, step], y_pred[:, step])\n",
    "        print(f\"Step {step + 1} MAE: {step_mae:.2f}, MSE: {step_mse:.2f}\")\n",
    "\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(\n",
    "    y_pred_scaled.reshape(-1, future_label))  # 形状恢复为 (n_samples, 5)\n",
    "print(y_test.shape, y_pred.shape)\n",
    "\n",
    "evaluate_predictions(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
